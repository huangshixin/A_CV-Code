分布式训练的注意事项
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
导入相应的包
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
import torch.distributed as dist
import os,time

'''
As of PyTorch v1.8, Windows supports all collective communications backend but NCCL, If the init_method argument of init_process_group() points to a file it must adhere to the following schema:

    Local file system, init_method="file:///d:/tmp/some_file"

    Shared file system, init_method="file://////{machine_name}/{share_folder_name}/some_file"

Same as on Linux platform, you can enable TcpStore by setting environment variables, MASTER_ADDR and MASTER_PORT.
'''
