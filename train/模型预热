一、什么是模型预热？
模型预热是一种训练思想，在模型预训练阶段，使用较小的学习率训练一些epochs或者step，目的是为了训练出一个学习率，然后采用新的学习率进行训练。
能提高模型的精度。

二、为什么要使用warmup？
刚开始训练的时候，模型的权重是随机初始化的，模型对数据的“理解程度”接近为0，如果刚开始采用较大的学习率，可能会带来模型的不稳定(震荡)，之后再使用预先设置的学习率进行训练，使得模型收敛速度变得更快，模型收敛效果更佳。最后，使用小学习率继续探索，如果步子太大，容易错过局部最优点。


三、Warmup的实现方法

1、gradual warmup

18年Facebook提出了gradual warmup来解决这个问题，即从最初的小学习率开始，每个step增大一点点，直到达到最初设置的比较大的学习率时，采用最初设置的学习率进行训练。gradual warmup的实现模拟代码如下:

"""
Implements gradual warmup, if train_steps < warmup_steps, the
learning rate will be `train_steps/warmup_steps * init_lr`.
Args:
    warmup_steps:warmup步长阈值,即train_steps<warmup_steps,使用预热学习率,否则使用预设值学习率
    train_steps:训练了的步长数
    init_lr:预设置学习率
"""
import numpy as np
warmup_steps = 2500
init_lr = 0.1  
# 模拟训练15000步
max_steps = 15000
for train_steps in range(max_steps):
    if warmup_steps and train_steps < warmup_steps:
        warmup_percent_done = train_steps / warmup_steps
        warmup_learning_rate = init_lr * warmup_percent_done  #gradual warmup_lr
        learning_rate = warmup_learning_rate
    else:
        #learning_rate = np.sin(learning_rate)  #预热学习率结束后,学习率呈sin衰减
        learning_rate = learning_rate**1.0001 #预热学习率结束后,学习率呈指数衰减(近似模拟指数衰减)
    if (train_steps+1) % 100 == 0:
             print("train_steps:%.3f--warmup_steps:%.3f--learning_rate:%.3f" % (
                 train_steps+1,warmup_steps,learning_rate))





