--nproc_per_node GPU个数
--master_port 12345 端口号
--cfg <config-file> yaml文件
resume swin_base_patch4_window7_224.pth模型的包
--data-path <imagenet-path> 数据集地址



进行模型评估
eval

python -m torch.distributed.launch
--nproc_per_node <num-of-gpus-to-use> --master_port 12345 main.py --eval \
--cfg <config-file> --resume <checkpoint> --data-path <imagenet-path>



-----单卡预测----
无幕艳意:
single eval

无幕艳意:
python -m torch.distributed.launch --nproc_per_node 1 --master_port 12345 main.py --eval \
--cfg configs/swin_base_patch4_window7_224.yaml --resume swin_base_patch4_window7_224.pth --data-path <imagenet-path>
-------------------------------------------




无幕艳意:
train

无幕艳意:
python -m torch.distributed.launch --nproc_per_node <num-of-gpus-to-use> --master_port 12345  main.py \
--cfg <config-file> --data-path <imagenet-path> [--batch-size <batch-size-per-gpu> --output <output-directory> --tag <job-tag>]

无幕艳意:
When GPU memory is not enough, you can try the following suggestions:
Use gradient accumulation by adding --accumulation-steps <steps>, set appropriate <steps> according to your need.

Use gradient checkpointing by adding --use-checkpoint, e.g., it saves about 60% memory when training Swin-B. Please refer to this page for more details.
We recommend using multi-node with more GPUs for training very large models, a tutorial can be found in this page.
To change config options in general, you can use --opts KEY1 VALUE1 KEY2 VALUE2, e.g., --opts TRAIN.EPOCHS 100 TRAIN.WARMUP_EPOCHS 5 will change total epochs to 100 and warm-up epochs to 5.
For additional options, see config and run python main.py --help to get detailed message.


无幕艳意:
python -m torch.distributed.launch --nproc_per_node 8 --master_port 12345  main.py \
--cfg configs/swin_tiny_patch4_window7_224.yaml --data-path <imagenet-path> --batch-size 128

无幕艳意:
python -m torch.distributed.launch --nproc_per_node 8 --master_port 12345  main.py \
--cfg configs/swin_base_patch4_window7_224.yaml --data-path <imagenet-path> --batch-size 64 \
--accumulation-steps 2 [--use-checkpoint]

无幕艳意:
python -m torch.distributed.launch --nproc_per_node 1 --master_port 12345  main.py \
--cfg <config-file> --data-path <imagenet-path> --batch-size 64 --throughput --amp-opt-level O0
